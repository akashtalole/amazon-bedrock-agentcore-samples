{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "746626f6b18e1c8c",
   "metadata": {},
   "source": [
    "## Advanced Data Analysis using Amazon AgentCore Bedrock Code Interpreter- Tutorial(Strands)\n",
    "This tutorial demonstrates how to create an AI agent that performs advanced data analysis through code execution using Python. We use Amazon Bedrock AgentCore Code Interpreter to run code that is generated by the LLM.\n",
    "\n",
    "This tutorial demonstrates how to use AgentCore Bedrock Code Interpreter to:\n",
    "1. Set up a sandbox environment\n",
    "2. Configure a strands based agent that performs advanced data analysis by generating code based on the user query\n",
    "3. Execute code in a sandbox environment using Code Interpreter\n",
    "4. Display the results back to the user\n",
    "\n",
    "## Prerequisites\n",
    "- AWS account with Bedrock AgentCore Code Interpreter access\n",
    "- You have the necessary IAM permissions to create and manage code interpreter resources\n",
    "- Required Python packages installed(including boto3, bedrock-agentcore & strands)\n",
    "- IAM role should have permissions to invoke models on Amazon Bedrock\n",
    " - Access to Claude Sonnet 3.7 & Claude Sonnet 4 models in the US Oregon (us-west-2) region (Claude Sonnet 4 is the default model for Strands SDK)\n",
    "\n",
    "## Your IAM execution role should have the following IAM policy attached"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f323388415caf3f7",
   "metadata": {},
   "source": [
    "~~~ {\n",
    "\"Version\": \"2012-10-17\",\n",
    "\"Statement\": [\n",
    "    {\n",
    "        \"Effect\": \"Allow\",\n",
    "        \"Action\": [\n",
    "            \"bedrock-agentcore:CreateCodeInterpreter\",\n",
    "            \"bedrock-agentcore:StartCodeInterpreterSession\",\n",
    "            \"bedrock-agentcore:InvokeCodeInterpreter\",\n",
    "            \"bedrock-agentcore:StopCodeInterpreterSession\",\n",
    "            \"bedrock-agentcore:DeleteCodeInterpreter\",\n",
    "            \"bedrock-agentcore:ListCodeInterpreters\",\n",
    "            \"bedrock-agentcore:GetCodeInterpreter\"\n",
    "        ],\n",
    "        \"Resource\": \"*\"\n",
    "    },\n",
    "    {\n",
    "        \"Effect\": \"Allow\",\n",
    "        \"Action\": [\n",
    "            \"logs:CreateLogGroup\",\n",
    "            \"logs:CreateLogStream\",\n",
    "            \"logs:PutLogEvents\"\n",
    "        ],\n",
    "        \"Resource\": \"arn:aws:logs:*:*:log-group:/aws/bedrock-agentcore/code-interpreter*\"\n",
    "    }\n",
    "]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226b2cb86ff18d9c",
   "metadata": {},
   "source": [
    "## How it works\n",
    "\n",
    "The code execution sandbox enables agents to safely process user queries by creating an isolated environment with a code interpreter, shell, and file system. After a Large Language Model helps with tool selection, code is executed within this session, before being returned to the user or Agent for synthesis.\n",
    "\n",
    "![architecture local](code-interpreter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859482709c77b03d",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Environment\n",
    "\n",
    "First, let's import the necessary libraries and initialize our Code Interpreter client.\n",
    "\n",
    "The default session timeout is 900 seconds(15 minutes). However, we start the session with a slightly session timeout duration of 1200 seconds(20 minutes), since we will perform detailed analysis on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13da423bac8ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb006310a96750c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:35:48.346322Z",
     "start_time": "2025-07-13T09:35:46.244470Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01K0T8P1E9ZRECW22PCHG81Z5C'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bedrock_agentcore.tools.code_interpreter_client import CodeInterpreter\n",
    "from strands import Agent, tool\n",
    "from strands.models import BedrockModel\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# Initialize the Code Interpreter within a supported AWS region.\n",
    "code_client = CodeInterpreter('us-west-2')\n",
    "code_client.start(session_timeout_seconds=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd02b57bd10dda2",
   "metadata": {},
   "source": [
    "## 2. Reading Local Data File\n",
    "\n",
    "Now we'll read the contents of our sample data file. The file consists of random data with 4 columns: Name, Preferred_City, Preferred_Animal, Preferred_Thing and ~ 300,000 records.\n",
    "\n",
    "We will analyze this file using an agent little later, to understand distributions and outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bef3821f290a589b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:35:48.465278Z",
     "start_time": "2025-07-13T09:35:48.385287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Preferred_City</th>\n",
       "      <th>Preferred_Animal</th>\n",
       "      <th>Preferred_Thing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Betty Ramirez</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>Elephant</td>\n",
       "      <td>Sofa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jennifer Green</td>\n",
       "      <td>Naples</td>\n",
       "      <td>Bee</td>\n",
       "      <td>Shirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John Lopez</td>\n",
       "      <td>Helsinki</td>\n",
       "      <td>Zebra</td>\n",
       "      <td>Wallet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Susan Gonzalez</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>Chicken</td>\n",
       "      <td>Phone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jennifer Wright</td>\n",
       "      <td>Buenos Aires</td>\n",
       "      <td>Goat</td>\n",
       "      <td>Wallet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name Preferred_City Preferred_Animal Preferred_Thing\n",
       "0    Betty Ramirez         Dallas         Elephant            Sofa\n",
       "1   Jennifer Green         Naples              Bee           Shirt\n",
       "2       John Lopez       Helsinki            Zebra          Wallet\n",
       "3   Susan Gonzalez        Beijing          Chicken           Phone\n",
       "4  Jennifer Wright   Buenos Aires             Goat          Wallet"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_csv(\"samples/data.csv\")\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38277480cbc38ba0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:35:48.503763Z",
     "start_time": "2025-07-13T09:35:48.495825Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_file(file_path: str) -> str:\n",
    "    \"\"\"Helper function to read file content with error handling\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{file_path}' was not found.\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "data_file_content = read_file(\"samples/data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fc91e0fb83fa05",
   "metadata": {},
   "source": [
    "## 3. Preparing Files for Sandbox Environment\n",
    "\n",
    "We'll create a structure that defines the files we want to create in the sandbox environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da44fb745b84c6ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:35:49.703849Z",
     "start_time": "2025-07-13T09:35:49.699079Z"
    }
   },
   "outputs": [],
   "source": [
    "files_to_create = [\n",
    "                {\n",
    "                    \"path\": \"data.csv\",\n",
    "                    \"text\": data_file_content\n",
    "                }]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f055bea34c93279",
   "metadata": {},
   "source": [
    "## 4. Creating Helper Function for Tool Invocation\n",
    "\n",
    "This helper function will make it easier to call sandbox tools and handle their responses. Within an active session, you can execute code in supported languages (Python, JavaScript), access libraries based on your dependencies configuration, generate visualizations, and maintain state between executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a74164c54b3b8ad5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:35:50.359366Z",
     "start_time": "2025-07-13T09:35:50.356755Z"
    }
   },
   "outputs": [],
   "source": [
    "def call_tool(tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Helper function to invoke sandbox tools\n",
    "\n",
    "    Args:\n",
    "        tool_name (str): Name of the tool to invoke\n",
    "        arguments (Dict[str, Any]): Arguments to pass to the tool\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: JSON formatted result\n",
    "    \"\"\"\n",
    "    response = code_client.invoke(tool_name, arguments)\n",
    "    for event in response[\"stream\"]:\n",
    "        return json.dumps(event[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd33790785ac084a",
   "metadata": {},
   "source": [
    "## 5. Write data file to Code Sandbox\n",
    "\n",
    "Now we'll write our data file into the sandbox environment and verify they were created successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "380afae3a5ba4934",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:36:00.346136Z",
     "start_time": "2025-07-13T09:35:50.965773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing files result:\n",
      "{\"content\": [{\"type\": \"text\", \"text\": \"Successfully wrote all 1 files\"}], \"isError\": false}\n",
      "\n",
      "Files in sandbox:\n",
      "{\"content\": [{\"type\": \"resource_link\", \"uri\": \"file:///log\", \"name\": \"log\", \"description\": \"Directory\"}, {\"type\": \"resource_link\", \"mimeType\": \"text/csv\", \"uri\": \"file:///data.csv\", \"name\": \"data.csv\", \"description\": \"File\"}, {\"type\": \"resource_link\", \"uri\": \"file:///.ipython\", \"name\": \".ipython\", \"description\": \"Directory\"}], \"isError\": false}\n"
     ]
    }
   ],
   "source": [
    "# Write files to sandbox\n",
    "writing_files = call_tool(\"writeFiles\", {\"content\": files_to_create})\n",
    "print(\"Writing files result:\")\n",
    "print(writing_files)\n",
    "\n",
    "# Verify files were created\n",
    "listing_files = call_tool(\"listFiles\", {\"path\": \"\"})\n",
    "print(\"\\nFiles in sandbox:\")\n",
    "print(listing_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640eae7a52ce9f8d",
   "metadata": {},
   "source": [
    "## 6. Perform Advanced Analysis using Strands based Agent\n",
    "\n",
    "Now we will configure an agent to perform data analysis on the data file that we uploaded into the sandbox(above)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3b068faf4a5eaa",
   "metadata": {},
   "source": [
    "### 6.1 System Prompt Definition\n",
    "Define the behavior and capabilities of the AI assistant. We instruct our assistant to always validate answers through code execution and data based reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e6830a170b45ce3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:36:00.366216Z",
     "start_time": "2025-07-13T09:36:00.364374Z"
    }
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant that validates all answers through code execution using the tools provided. DO NOT Answer questions without using the tools\n",
    "\n",
    "VALIDATION PRINCIPLES:\n",
    "1. When making claims about code, algorithms, or calculations - write code to verify them\n",
    "2. Use execute_python to test mathematical calculations, algorithms, and logic\n",
    "3. Create test scripts to validate your understanding before giving answers\n",
    "4. Always show your work with actual code execution\n",
    "5. If uncertain, explicitly state limitations and validate what you can\n",
    "\n",
    "APPROACH:\n",
    "- If asked about a programming concept, implement it in code to demonstrate\n",
    "- If asked for calculations, compute them programmatically AND show the code\n",
    "- If implementing algorithms, include test cases to prove correctness\n",
    "- Document your validation process for transparency\n",
    "- The sandbox maintains state between executions, so you can refer to previous results\n",
    "\n",
    "TOOL AVAILABLE:\n",
    "- execute_python: Run Python code and see output\n",
    "\n",
    "RESPONSE FORMAT: The execute_python tool returns a JSON response with:\n",
    "- sessionId: The sandbox session ID\n",
    "- id: Request ID\n",
    "- isError: Boolean indicating if there was an error\n",
    "- content: Array of content objects with type and text/data\n",
    "- structuredContent: For code execution, includes stdout, stderr, exitCode, executionTime\n",
    "\n",
    "For successful code execution, the output will be in content[0].text and also in structuredContent.stdout.\n",
    "Check isError field to see if there was an error.\n",
    "\n",
    "Be thorough, accurate, and always validate your answers when possible.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87157a0ea835ab5",
   "metadata": {},
   "source": [
    "### 6.2 Code Execution Tool Definition\n",
    "Next we define the function as tool that will be used by the Agent as tool, to run code in the code sandbox. We use the @tool decorator to annotate the function as a custom tool for the Agent.\n",
    "\n",
    "Within an active code interpreter session, you can execute code in supported languages (Python, JavaScript), access libraries based on your dependencies configuration, generate visualizations, and maintain state between executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "750472cd96e873c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:36:34.464620Z",
     "start_time": "2025-07-13T09:36:34.457484Z"
    }
   },
   "outputs": [],
   "source": [
    "#Define and configure the code interpreter tool\n",
    "@tool\n",
    "def execute_python(code: str, description: str = \"\") -> str:\n",
    "    \"\"\"Execute Python code in the sandbox.\"\"\"\n",
    "\n",
    "    if description:\n",
    "        code = f\"# {description}\\n{code}\"\n",
    "\n",
    "    #Print generated Code to be executed\n",
    "    print(f\"\\n Generated Code: {code}\")\n",
    "\n",
    "\n",
    "    # Call the Invoke method and execute the generated code, within the initialized code interpreter session\n",
    "    response = code_client.invoke(\"executeCode\", {\n",
    "        \"code\": code,\n",
    "        \"language\": \"python\",\n",
    "        \"clearContext\": False\n",
    "    })\n",
    "    for event in response[\"stream\"]:\n",
    "        return json.dumps(event[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bc47b82755730d",
   "metadata": {},
   "source": [
    "### 6.3 Agent Configuration\n",
    "We create and configure an agent using the Strands SDK. We provide it the system prompt and the tool we defined above to execute generate code.\n",
    "\n",
    "We also override Strands SDK's default model provider (Claude Sonnet 4) with Claude Sonnet 3.7. To use the Claude Sonnet 3.7 model we specify the [Cross-region Inference (CRIS)](https://docs.aws.amazon.com/bedrock/latest/userguide/cross-region-inference.html) profile id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14c5e8f18b70dc01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:36:35.341080Z",
     "start_time": "2025-07-13T09:36:35.239620Z"
    }
   },
   "outputs": [],
   "source": [
    "model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "model= BedrockModel(model_id=model_id)\n",
    "\n",
    "#configure the strands agent including the model and tool(s)\n",
    "agent=Agent(\n",
    "    model=model,\n",
    "        tools=[execute_python],\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        callback_handler=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25693e10aa1e5689",
   "metadata": {},
   "source": [
    "## 7. Agent Invocation and Response Processing\n",
    "We invoke the agent with our query and process the agent's response\n",
    "\n",
    "\n",
    "Note: Async execution requires running in an async environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddfa7cb97ead950",
   "metadata": {},
   "source": [
    "## 7.1 Query to perform Exploratory Data Analysis(EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f98916e2cc3627",
   "metadata": {},
   "source": [
    "Let's start with a query which instructs the agent to perform exploratory data analysis on the data file in the code sandbox environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7370ff964d06a1cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:40:07.612284Z",
     "start_time": "2025-07-13T09:37:28.402310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll help you perform exploratory data analysis on the 'data.csv' file. Let's first check if the file exists in the sandbox environment and then analyze its contents.\n",
      " Generated Code: # Checking if data.csv exists\n",
      "# First, let's check if the file exists\n",
      "import os\n",
      "if os.path.exists('data.csv'):\n",
      "    print(\"File 'data.csv' exists. Let's proceed with the analysis.\")\n",
      "else:\n",
      "    print(\"File 'data.csv' does not exist in the current directory.\")\n",
      "    print(\"Current directory contains these files:\")\n",
      "    print(os.listdir())\n",
      "Great! The file exists. Let's load the data and get a basic understanding of its structure and content:\n",
      " Generated Code: # Loading the dataset and displaying basic information\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Load the data\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Display basic information\n",
      "print(\"Dataset shape:\", df.shape)\n",
      "print(\"\\nFirst 5 rows:\")\n",
      "print(df.head())\n",
      "print(\"\\nData types:\")\n",
      "print(df.dtypes)\n",
      "print(\"\\nSummary statistics:\")\n",
      "print(df.describe())\n",
      "print(\"\\nMissing values:\")\n",
      "print(df.isnull().sum())\n",
      "I see the seaborn library isn't available in this environment. Let's proceed without it:\n",
      " Generated Code: # Loading the dataset and displaying basic information\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Load the data\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Display basic information\n",
      "print(\"Dataset shape:\", df.shape)\n",
      "print(\"\\nFirst 5 rows:\")\n",
      "print(df.head())\n",
      "print(\"\\nData types:\")\n",
      "print(df.dtypes)\n",
      "print(\"\\nSummary statistics:\")\n",
      "print(df.describe())\n",
      "print(\"\\nMissing values:\")\n",
      "print(df.isnull().sum())\n",
      "Now let's analyze the distributions of the categorical variables and look for any patterns:\n",
      " Generated Code: # Analyzing distributions of categorical variables\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Load the data\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Function to plot top N categories\n",
      "def plot_top_categories(series, column_name, n=10):\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    value_counts = series.value_counts().head(n)\n",
      "    value_counts.plot(kind='bar')\n",
      "    plt.title(f'Top {n} {column_name}')\n",
      "    plt.ylabel('Count')\n",
      "    plt.xlabel(column_name)\n",
      "    plt.xticks(rotation=45, ha='right')\n",
      "    plt.tight_layout()\n",
      "    plt.show()\n",
      "\n",
      "# Analyze distributions of each categorical variable\n",
      "print(\"========== Name Distribution (top 15) ==========\")\n",
      "name_counts = df['Name'].value_counts().head(15)\n",
      "print(name_counts)\n",
      "print(f\"\\nTotal unique names: {df['Name'].nunique()}\")\n",
      "\n",
      "print(\"\\n========== Preferred City Distribution (top 15) ==========\")\n",
      "city_counts = df['Preferred_City'].value_counts().head(15)\n",
      "print(city_counts)\n",
      "print(f\"\\nTotal unique cities: {df['Preferred_City'].nunique()}\")\n",
      "\n",
      "print(\"\\n========== Preferred Animal Distribution (top 15) ==========\")\n",
      "animal_counts = df['Preferred_Animal'].value_counts().head(15)\n",
      "print(animal_counts)\n",
      "print(f\"\\nTotal unique animals: {df['Preferred_Animal'].nunique()}\")\n",
      "\n",
      "print(\"\\n========== Preferred Thing Distribution (top 15) ==========\")\n",
      "thing_counts = df['Preferred_Thing'].value_counts().head(15)\n",
      "print(thing_counts)\n",
      "print(f\"\\nTotal unique things: {df['Preferred_Thing'].nunique()}\")\n",
      "\n",
      "# Check for any unusual distributions or outliers\n",
      "print(\"\\n========== Distribution Statistics ==========\")\n",
      "for column in df.columns:\n",
      "    print(f\"\\n{column} distribution:\")\n",
      "    counts = df[column].value_counts()\n",
      "    min_count = counts.min()\n",
      "    max_count = counts.max()\n",
      "    mean_count = counts.mean()\n",
      "    std_count = counts.std()\n",
      "    \n",
      "    print(f\"Min frequency: {min_count}\")\n",
      "    print(f\"Max frequency: {max_count} (for '{counts.idxmax()}')\")\n",
      "    print(f\"Mean frequency: {mean_count:.2f}\")\n",
      "    print(f\"Std Dev: {std_count:.2f}\")\n",
      "    print(f\"Range (max/min ratio): {max_count/min_count:.2f}\")\n",
      "Let's look for potential associations between variables:\n",
      " Generated Code: # Analyzing associations between variables\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "# Load the data\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Analyze combinations of preferences\n",
      "print(\"========== Top combinations ==========\")\n",
      "combinations = df.groupby(['Preferred_City', 'Preferred_Animal', 'Preferred_Thing']).size().reset_index(name='count')\n",
      "combinations = combinations.sort_values('count', ascending=False)\n",
      "\n",
      "print(\"Top 10 combinations of city, animal, and thing:\")\n",
      "print(combinations.head(10))\n",
      "\n",
      "# Check if there are any strong associations between pairs of preferences\n",
      "print(\"\\n========== Associations between preferences ==========\")\n",
      "\n",
      "# Check city-animal associations\n",
      "city_animal = pd.crosstab(df['Preferred_City'], df['Preferred_Animal'])\n",
      "# Calculate chi-square and p-value\n",
      "from scipy.stats import chi2_contingency\n",
      "chi2, p, dof, expected = chi2_contingency(city_animal)\n",
      "print(f\"Chi-square test for city and animal association: chi2={chi2:.2f}, p={p}\")\n",
      "\n",
      "# Check city-thing associations\n",
      "city_thing = pd.crosstab(df['Preferred_City'], df['Preferred_Thing'])\n",
      "chi2, p, dof, expected = chi2_contingency(city_thing)\n",
      "print(f\"Chi-square test for city and thing association: chi2={chi2:.2f}, p={p}\")\n",
      "\n",
      "# Check animal-thing associations\n",
      "animal_thing = pd.crosstab(df['Preferred_Animal'], df['Preferred_Thing'])\n",
      "chi2, p, dof, expected = chi2_contingency(animal_thing)\n",
      "print(f\"Chi-square test for animal and thing association: chi2={chi2:.2f}, p={p}\")\n",
      "\n",
      "# Let's check if any specific name has unusual preferences\n",
      "print(\"\\n========== Name-based preference patterns ==========\")\n",
      "# Get the top 5 names\n",
      "top_names = df['Name'].value_counts().head(5).index.tolist()\n",
      "\n",
      "for name in top_names:\n",
      "    name_df = df[df['Name'] == name]\n",
      "    print(f\"\\nPreferences for {name} (out of {len(name_df)} occurrences):\")\n",
      "    \n",
      "    print(\"Top cities:\")\n",
      "    print(name_df['Preferred_City'].value_counts().head(3))\n",
      "    \n",
      "    print(\"Top animals:\")\n",
      "    print(name_df['Preferred_Animal'].value_counts().head(3))\n",
      "    \n",
      "    print(\"Top things:\")\n",
      "    print(name_df['Preferred_Thing'].value_counts().head(3))\n",
      "Let's visualize some of the distributions to better understand the data:\n",
      " Generated Code: # Visualizing distributions\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "# Load the data\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Create a figure with subplots\n",
      "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
      "\n",
      "# Plot top 10 cities\n",
      "city_counts = df['Preferred_City'].value_counts().head(10)\n",
      "axes[0, 0].bar(city_counts.index, city_counts.values)\n",
      "axes[0, 0].set_title('Top 10 Preferred Cities')\n",
      "axes[0, 0].set_ylabel('Count')\n",
      "axes[0, 0].tick_params(axis='x', rotation=45)\n",
      "\n",
      "# Plot top 10 animals\n",
      "animal_counts = df['Preferred_Animal'].value_counts().head(10)\n",
      "axes[0, 1].bar(animal_counts.index, animal_counts.values)\n",
      "axes[0, 1].set_title('Top 10 Preferred Animals')\n",
      "axes[0, 1].set_ylabel('Count')\n",
      "axes[0, 1].tick_params(axis='x', rotation=45)\n",
      "\n",
      "# Plot top 10 things\n",
      "thing_counts = df['Preferred_Thing'].value_counts().head(10)\n",
      "axes[1, 0].bar(thing_counts.index, thing_counts.values)\n",
      "axes[1, 0].set_title('Top 10 Preferred Things')\n",
      "axes[1, 0].set_ylabel('Count')\n",
      "axes[1, 0].tick_params(axis='x', rotation=45)\n",
      "\n",
      "# Plot top 10 names\n",
      "name_counts = df['Name'].value_counts().head(10)\n",
      "axes[1, 1].bar(name_counts.index, name_counts.values)\n",
      "axes[1, 1].set_title('Top 10 Names')\n",
      "axes[1, 1].set_ylabel('Count')\n",
      "axes[1, 1].tick_params(axis='x', rotation=45)\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('distributions.png')\n",
      "plt.close()\n",
      "\n",
      "# Check for outliers in the distributions\n",
      "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
      "\n",
      "# Function to create boxplots for counts\n",
      "def create_boxplot(data_series, ax, title):\n",
      "    counts = data_series.value_counts().values\n",
      "    ax.boxplot(counts)\n",
      "    ax.set_title(title)\n",
      "    ax.set_ylabel('Frequency')\n",
      "\n",
      "create_boxplot(df['Preferred_City'], axes[0, 0], 'City Frequencies')\n",
      "create_boxplot(df['Preferred_Animal'], axes[0, 1], 'Animal Frequencies')\n",
      "create_boxplot(df['Preferred_Thing'], axes[1, 0], 'Thing Frequencies')\n",
      "create_boxplot(df['Name'], axes[1, 1], 'Name Frequencies')\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('boxplots.png')\n",
      "plt.close()\n",
      "\n",
      "# Print key statistics about the distributions\n",
      "print(\"========== Distribution Analysis ==========\")\n",
      "\n",
      "for column in df.columns:\n",
      "    print(f\"\\n{column} frequency distribution:\")\n",
      "    counts = df[column].value_counts()\n",
      "    \n",
      "    # Calculate statistics\n",
      "    q1 = counts.quantile(0.25)\n",
      "    q3 = counts.quantile(0.75)\n",
      "    iqr = q3 - q1\n",
      "    lower_bound = q1 - 1.5 * iqr\n",
      "    upper_bound = q3 + 1.5 * iqr\n",
      "    \n",
      "    # Identify outliers\n",
      "    outliers = counts[(counts < lower_bound) | (counts > upper_bound)]\n",
      "    \n",
      "    print(f\"  Min: {counts.min()}\")\n",
      "    print(f\"  Q1: {q1}\")\n",
      "    print(f\"  Median: {counts.median()}\")\n",
      "    print(f\"  Q3: {q3}\")\n",
      "    print(f\"  Max: {counts.max()}\")\n",
      "    print(f\"  IQR: {iqr}\")\n",
      "    print(f\"  Lower bound for outliers: {lower_bound}\")\n",
      "    print(f\"  Upper bound for outliers: {upper_bound}\")\n",
      "    print(f\"  Number of outliers: {len(outliers)}\")\n",
      "    \n",
      "    if len(outliers) > 0:\n",
      "        print(\"  Outliers:\")\n",
      "        for item, count in outliers.items():\n",
      "            print(f\"    {item}: {count}\")\n",
      "\n",
      "print(\"\\nImages saved as 'distributions.png' and 'boxplots.png'\")\n",
      "Let's check for uniqueness of combinations and look for any interesting patterns:\n",
      " Generated Code: # Analyzing uniqueness and patterns in combinations\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from collections import Counter\n",
      "\n",
      "# Load the data\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Check how many unique combinations exist\n",
      "print(\"========== Unique Combinations Analysis ==========\")\n",
      "\n",
      "# Add a combined column\n",
      "df['Combined'] = df['Preferred_City'] + ' | ' + df['Preferred_Animal'] + ' | ' + df['Preferred_Thing']\n",
      "\n",
      "total_rows = len(df)\n",
      "unique_combinations = df['Combined'].nunique()\n",
      "print(f\"Total rows in dataset: {total_rows}\")\n",
      "print(f\"Unique combinations of city, animal, and thing: {unique_combinations}\")\n",
      "print(f\"Percentage of unique combinations: {unique_combinations/total_rows*100:.2f}%\")\n",
      "\n",
      "# Most common combinations\n",
      "print(\"\\nTop 15 most common combinations:\")\n",
      "combinations_counts = df['Combined'].value_counts().head(15)\n",
      "for combo, count in combinations_counts.items():\n",
      "    print(f\"{combo}: {count} occurrences\")\n",
      "\n",
      "# Check for any patterns in the data\n",
      "print(\"\\n========== Pattern Analysis ==========\")\n",
      "\n",
      "# Check if certain cities have strong preferences for specific animals\n",
      "city_animal = pd.crosstab(df['Preferred_City'], df['Preferred_Animal'], normalize='index')\n",
      "# Get top animal for each city\n",
      "top_animal_by_city = city_animal.idxmax(axis=1)\n",
      "print(\"\\nTop animal for each city:\")\n",
      "print(top_animal_by_city)\n",
      "\n",
      "# Check if certain cities have strong preferences for specific things\n",
      "city_thing = pd.crosstab(df['Preferred_City'], df['Preferred_Thing'], normalize='index')\n",
      "# Get top thing for each city\n",
      "top_thing_by_city = city_thing.idxmax(axis=1)\n",
      "print(\"\\nTop thing for each city:\")\n",
      "print(top_thing_by_city)\n",
      "\n",
      "# Check if certain animals have strong preferences for specific things\n",
      "animal_thing = pd.crosstab(df['Preferred_Animal'], df['Preferred_Thing'], normalize='index')\n",
      "# Get top thing for each animal\n",
      "top_thing_by_animal = animal_thing.idxmax(axis=1)\n",
      "print(\"\\nTop thing for each animal:\")\n",
      "print(top_thing_by_animal)\n",
      "\n",
      "# Check the randomness of the distributions\n",
      "print(\"\\n========== Randomness Analysis ==========\")\n",
      "\n",
      "# Calculate entropy for each category as a measure of randomness\n",
      "# Higher entropy means more uniform/random distribution\n",
      "from scipy.stats import entropy\n",
      "\n",
      "def calculate_entropy(series):\n",
      "    counts = series.value_counts()\n",
      "    probabilities = counts / counts.sum()\n",
      "    return entropy(probabilities)\n",
      "\n",
      "print(f\"Entropy of Name distribution: {calculate_entropy(df['Name']):.4f}\")\n",
      "print(f\"Entropy of Preferred_City distribution: {calculate_entropy(df['Preferred_City']):.4f}\")\n",
      "print(f\"Entropy of Preferred_Animal distribution: {calculate_entropy(df['Preferred_Animal']):.4f}\")\n",
      "print(f\"Entropy of Preferred_Thing distribution: {calculate_entropy(df['Preferred_Thing']):.4f}\")\n",
      "\n",
      "# Theoretical maximum entropy for each category (completely uniform distribution)\n",
      "import math\n",
      "max_entropy_name = math.log(df['Name'].nunique())\n",
      "max_entropy_city = math.log(df['Preferred_City'].nunique())\n",
      "max_entropy_animal = math.log(df['Preferred_Animal'].nunique())\n",
      "max_entropy_thing = math.log(df['Preferred_Thing'].nunique())\n",
      "\n",
      "print(f\"\\nMaximum possible entropy (if perfectly uniform):\")\n",
      "print(f\"Name: {max_entropy_name:.4f}\")\n",
      "print(f\"Preferred_City: {max_entropy_city:.4f}\")\n",
      "print(f\"Preferred_Animal: {max_entropy_animal:.4f}\")\n",
      "print(f\"Preferred_Thing: {max_entropy_thing:.4f}\")\n",
      "\n",
      "print(f\"\\nEntropy ratio (actual/maximum - higher means more uniform distribution):\")\n",
      "print(f\"Name: {calculate_entropy(df['Name'])/max_entropy_name:.4f}\")\n",
      "print(f\"Preferred_City: {calculate_entropy(df['Preferred_City'])/max_entropy_city:.4f}\")\n",
      "print(f\"Preferred_Animal: {calculate_entropy(df['Preferred_Animal'])/max_entropy_animal:.4f}\")\n",
      "print(f\"Preferred_Thing: {calculate_entropy(df['Preferred_Thing'])/max_entropy_thing:.4f}\")\n",
      "Based on the comprehensive exploratory data analysis (EDA) I've conducted on the 'data.csv' file, here's a summary of the key findings:\n",
      "\n",
      "## Dataset Overview\n",
      "- **Size**: 299,130 records with 4 columns\n",
      "- **Columns**: Name, Preferred_City, Preferred_Animal, Preferred_Thing\n",
      "- **Data types**: All columns are categorical (object type)\n",
      "- **Missing values**: None detected\n",
      "\n",
      "## Distributions\n",
      "\n",
      "### Names\n",
      "- There are 1,722 unique names in the dataset\n",
      "- The most common name is \"Lisa White\" (222 occurrences)\n",
      "- The name distribution has several outliers:\n",
      "  - **High frequency outliers**: Lisa White, Sarah Perez, Susan Green, and others exceed the upper bound of 207.5 occurrences\n",
      "  - **Low frequency outliers**: Donald Jackson (120 occurrences), Linda Allen (132), etc.\n",
      "- Most names appear approximately 174 times (median)\n",
      "\n",
      "### Preferred Cities\n",
      "- There are 55 unique cities\n",
      "- Most popular city: Prague (5,587 occurrences)\n",
      "- The distribution is remarkably uniform with no statistical outliers\n",
      "- Cities have very similar frequencies (all around 5,400-5,500)\n",
      "\n",
      "### Preferred Animals\n",
      "- There are 50 unique animals\n",
      "- Most popular animal: Goat (6,141 occurrences)\n",
      "- Only one outlier detected: Shark (5,761 occurrences) - slightly lower than expected\n",
      "- Other popular animals include: Pig, Hamster, Wolf, and Panda\n",
      "\n",
      "### Preferred Things\n",
      "- There are 51 unique things\n",
      "- Most popular thing: Pencil (6,058 occurrences)\n",
      "- No statistical outliers detected\n",
      "- Distribution is very uniform (all items have frequencies around 5,800-6,000)\n",
      "\n",
      "## Associations & Patterns\n",
      "\n",
      "### Combinations\n",
      "- There are 123,647 unique combinations of city, animal, and thing\n",
      "- This represents 41.34% of all records\n",
      "- Most common combination is \"Berlin | Whale | Pants\" with only 10 occurrences\n",
      "- Combinations are quite diverse, indicating little correlation between preferences\n",
      "\n",
      "### Statistical Tests\n",
      "- Chi-square tests showed no significant associations between:\n",
      "  - City and animal preferences (p = 0.94)\n",
      "  - City and thing preferences (p = 0.43)\n",
      "  - Animal and thing preferences (p = 0.12)\n",
      "\n",
      "### Top Preferences Analysis\n",
      "- Each city, animal, and thing combination shows minimal patterns\n",
      "- For example, Prague prefers Deer and Cup, Berlin prefers Dog and Wallet, etc.\n",
      "- These slight preferences are not statistically significant\n",
      "\n",
      "### Randomness Analysis\n",
      "- Entropy calculations show extremely high randomness in distributions:\n",
      "  - All categorical columns (except Name) have entropy ratios of almost exactly 1.0\n",
      "  - This indicates nearly perfect uniform distributions\n",
      "  - Even Name has a very high entropy ratio of 0.9996\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "The data appears to be largely random with almost perfectly uniform distributions across cities, animals, and things. The only column showing slight non-uniformity is \"Name\", where a few names appear more frequently than others. The high entropy values and lack of significant patterns suggest this dataset may be synthetic or randomly generated.\n",
      "\n",
      "The absence of strong associations between the categorical variables (confirmed by chi-square tests) indicates that preferences for cities, animals, and things are independent of each other. The slight differences in frequencies don't represent meaningful patterns but rather expected statistical fluctuations in a large dataset."
     ]
    }
   ],
   "source": [
    "query = \"Load the file 'data.csv' and perform exploratory data analysis(EDA) on it. Tell me about distributions and outlier values.\"\n",
    "\n",
    "# Invoke the agent asynchcronously and stream the response\n",
    "response_text = \"\"\n",
    "async for event in agent.stream_async(query):\n",
    "    if \"data\" in event:\n",
    "        # Stream text response\n",
    "        chunk = event[\"data\"]\n",
    "        response_text += chunk\n",
    "        print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accab0cfe53ade15",
   "metadata": {},
   "source": [
    "## 7.2 Query to extract information\n",
    "\n",
    "Now, let's instruct the agent to extract specific information from the data file in the code sandbox environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f091d1f87558bd41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:37:07.283968Z",
     "start_time": "2025-07-13T09:36:45.865171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll help you find out how many individuals with the first name 'Kimberly' have 'Crocodile' as their favorite animal in the data.csv file. Let's write code to check this:\n",
      " Generated Code: # Count Kimberlys who prefer Crocodiles\n",
      "import pandas as pd\n",
      "\n",
      "# Load the dataset\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Filter for individuals with first name 'Kimberly' and favorite animal 'Crocodile'\n",
      "# First, let's split the Name column to get just the first name\n",
      "df['First_Name'] = df['Name'].str.split().str[0]\n",
      "\n",
      "# Now filter for Kimberly who likes Crocodiles\n",
      "kimberly_crocodile = df[(df['First_Name'] == 'Kimberly') & (df['Preferred_Animal'] == 'Crocodile')]\n",
      "\n",
      "# Count and display results\n",
      "count = len(kimberly_crocodile)\n",
      "print(f\"Number of individuals named Kimberly who prefer Crocodiles: {count}\")\n",
      "\n",
      "# If there are any, display their full information\n",
      "if count > 0:\n",
      "    print(\"\\nDetails of these individuals:\")\n",
      "    print(kimberly_crocodile)\n",
      "Let's also check the total number of individuals with the first name 'Kimberly' to understand this in context:\n",
      " Generated Code: # Count total Kimberlys and their animal preferences\n",
      "import pandas as pd\n",
      "\n",
      "# Load the dataset\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Get first names\n",
      "if 'First_Name' not in df.columns:\n",
      "    df['First_Name'] = df['Name'].str.split().str[0]\n",
      "\n",
      "# Count total Kimberlys\n",
      "total_kimberly = df[df['First_Name'] == 'Kimberly'].shape[0]\n",
      "print(f\"Total number of individuals named Kimberly: {total_kimberly}\")\n",
      "\n",
      "# Check animal preferences among Kimberlys\n",
      "kimberly_animals = df[df['First_Name'] == 'Kimberly']['Preferred_Animal'].value_counts()\n",
      "print(\"\\nAnimal preferences among individuals named Kimberly:\")\n",
      "print(kimberly_animals)\n",
      "\n",
      "# Calculate percentage for Crocodile\n",
      "crocodile_percentage = (kimberly_animals['Crocodile'] / total_kimberly) * 100\n",
      "print(f\"\\nPercentage of Kimberlys who prefer Crocodiles: {crocodile_percentage:.2f}%\")\n",
      "Based on the analysis of the 'data.csv' file, there are **120 individuals with the first name 'Kimberly' who have 'Crocodile' as their favorite animal**.\n",
      "\n",
      "Additional context:\n",
      "- There are a total of 7,149 individuals named Kimberly in the dataset\n",
      "- Among Kimberlys, Crocodile is actually the least preferred animal (1.68% of all Kimberlys)\n",
      "- The most preferred animal among Kimberlys is Cow (162 individuals)\n",
      "- The animal preferences among people named Kimberly appear to be fairly evenly distributed, with counts ranging from 120 to 162 for different animals\n",
      "\n",
      "This analysis confirms the pattern we observed in our earlier exploratory data analysis, where the distributions in the dataset tend to be quite uniform with only minor variations."
     ]
    }
   ],
   "source": [
    "query = \"Within the file 'data.csv', how many individuals with the first name 'Kimberly' have 'Crocodile' as their favourite animal?\"\n",
    "\n",
    "# Invoke the agent asynchcronously and stream the response\n",
    "response_text = \"\"\n",
    "async for event in agent.stream_async(query):\n",
    "    if \"data\" in event:\n",
    "        # Stream text response\n",
    "        chunk = event[\"data\"]\n",
    "        response_text += chunk\n",
    "        print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6b3ce0963d4a83",
   "metadata": {},
   "source": [
    "## 8. Cleanup\n",
    "\n",
    "Finally, we'll clean up by stopping the Code Interpreter session. Once finished using a session, the session should be shopped to release resources and avoid unnecessary charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "baa2ca7fce8b181d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:35:31.525724Z",
     "start_time": "2025-07-13T09:35:30.947964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code Interpreter session stopped successfully!\n"
     ]
    }
   ],
   "source": [
    "# Stop the Code Interpreter session\n",
    "code_client.stop()\n",
    "print(\"Code Interpreter session stopped successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
